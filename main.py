# -*- coding: utf-8 -*-
"""Youtube-comments-spam-classifier+LINKS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dkb8SOHsojtZh-QRE7OuZcBjMkk34ytk
"""

import matplotlib.pyplot as plt
from wordcloud import WordCloud
import pandas as pd
import numpy as np
import nltk
import warnings
import glob
import string
import re


from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from collections import Counter

#import sklearn packages for building classifiers
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score

import seaborn as sns
from bs4 import BeautifulSoup

"""# New Section"""

all_files = glob.glob("data/*.csv")
data = pd.concat((pd.read_csv(f, encoding="utf-8-sig") for f in all_files))
data.head()

def text_clear(text):
    text = re.sub(r'^https?:\/\/.*[\r\n]*', 'HTTPLINK', text) # replace all LINKS to one word to represent all link as one
    soup = BeautifulSoup(text)
    return soup.get_text().replace("\ufeff", "") # some of the words contains UTF-8 BOM mark, so we remove tags and this mark too

data = data.drop(["COMMENT_ID", "AUTHOR", "DATE"], axis=1)
data['CONTENT'] = data['CONTENT'].apply(text_clear)
data.head()

nltk.download("punkt")
nltk.download('stopwords')

warnings.filterwarnings('ignore')



#remove the punctuations and stopwords
def text_process(text):
    text = text.translate(str.maketrans('', '', string.punctuation)).lower()
    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]

    return " ".join(text)

data['CONTENT'] = data['CONTENT'].apply(text_process)

CONTENT = pd.DataFrame(data['CONTENT'])
CLASS = pd.DataFrame(data['CLASS'])

total_counts = Counter()
for i in range(len(CONTENT)):
    for word in CONTENT.values[i][0].split(" "):
        total_counts[word] += 1

print("Total words in data set: ", len(total_counts))

# Sorting in decreasing order (Word with highest frequency appears first)
vocab = sorted(total_counts, key=total_counts.get, reverse=True)
print(vocab[:60])

vocab_size = len(vocab)
word2idx = {}
#print vocab_size
for i, word in enumerate(vocab):
    word2idx[word] = i

# Text to Vector
def text_to_vector(text):
    word_vector = np.zeros(vocab_size)
    for word in text.split(" "):
        if word2idx.get(word) is None:
            continue
        else:
            word_vector[word2idx.get(word)] += 1
    return np.array(word_vector)

# Convert all titles to vectors
word_vectors = np.zeros((len(CONTENT), len(vocab)), dtype=np.int_)
for i, (_, text_) in enumerate(CONTENT.iterrows()):
    word_vectors[i] = text_to_vector(text_[0])

vectorizer = TfidfVectorizer()
vectors = vectorizer.fit_transform(data['CONTENT'])

#features = word_vectors
features = vectors

X_train, X_test, y_train, y_test = train_test_split(features, data['CLASS'], test_size=0.15, random_state=99) #

#initialize multiple classification models
svc = SVC(kernel='sigmoid', gamma=1.0)
knc = KNeighborsClassifier(n_neighbors=49)
mnb = MultinomialNB(alpha=0.2)
dtc = DecisionTreeClassifier(min_samples_split=7, random_state=99)
lrc = LogisticRegression(solver='liblinear', penalty='l1')
rfc = RandomForestClassifier(n_estimators=31, random_state=99)

#create a dictionary of variables and models
clfs = {'Support Vector' : svc,'KNeighbors' : knc, 'Multinomial Naive Bayes': mnb, 'Decision Tree': dtc, 'Logistic Regression': lrc, 'Random Forest': rfc}

#fit the data onto the models
def train(clf, features, targets):
    clf.fit(features, targets)

def predict(clf, features):
    return (clf.predict(features))

pred_scores_word_vectors = []

print("Confusion matrixes")
for k,v in clfs.items():
    train(v, X_train, y_train)
    pred = predict(v, X_test)
    
    pred_scores_word_vectors.append((k, accuracy_score(y_test , pred), precision_score(y_test, pred), recall_score(y_test, pred), f1_score(y_test, pred)))
    
    cm = confusion_matrix(y_test, pred)
    
    print("\n\nModel: ", k)
    print (pd.DataFrame(cm, columns=["Pred Ham", "Pred Spam"], index=[" Ham", "Spam"]))



print("\n\n", pd.DataFrame(pred_scores_word_vectors, columns=["Classification model", "Accuracy", "Precision Score", "Recall Score", "F1 Score"]))

